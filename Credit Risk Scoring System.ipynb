{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREDIT RISK SCORING SYSTEM\n",
    "\n",
    "### Problem Statement: Developing a Robust Credit Risk Scoring System using Machine Learning\n",
    "\n",
    "#### Context and Background\n",
    "\n",
    "In the rapidly evolving fintech industry, accurate credit risk assessment is paramount to financial institutions. Credit risk scoring is a critical tool that helps lenders evaluate the likelihood of a borrower defaulting on a loan. Traditional credit scoring models often rely on a limited set of financial indicators and can be prone to biases and inaccuracies. With the advent of big data and advanced machine learning techniques, there is a significant opportunity to enhance the accuracy and reliability of credit risk scoring models.\n",
    "\n",
    "#### Objective\n",
    "\n",
    "The primary objective of this project is to develop a robust, scalable, and interpretable machine learning model that predicts the probability of default for loan applicants. The model aims to leverage a wide array of features, including demographic data, financial status, credit history, and behavioral data, to deliver accurate credit risk assessments.\n",
    "\n",
    "#### Key Questions\n",
    "\n",
    "1. **Data Integration**: How can we effectively integrate diverse data sources to build a comprehensive dataset for credit risk assessment?\n",
    "2. **Feature Engineering**: What are the most predictive features for assessing credit risk, and how can we engineer new features to improve model performance?\n",
    "3. **Model Selection**: Which machine learning algorithms provide the best performance in terms of accuracy, interpretability, and computational efficiency for credit risk scoring?\n",
    "4. **Model Evaluation**: What metrics should be used to evaluate the performance of the credit risk model, and how can we ensure the model generalizes well to unseen data?\n",
    "5. **Bias and Fairness**: How can we detect and mitigate biases in the credit risk model to ensure fair treatment of all applicants?\n",
    "6. **Deployment and Monitoring**: How can we deploy the model in a real-world setting, and what mechanisms should be in place for continuous monitoring and updating of the model?\n",
    "\n",
    "#### Scope and Deliverables\n",
    "\n",
    "1. **Data Collection and Preprocessing**:\n",
    "   - Collect and preprocess data from the chosen datasets (Home Credit Default Risk, LendingClub Loan Data, and Give Me Some Credit).\n",
    "   - Handle missing values, outliers, and data inconsistencies.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**:\n",
    "   - Conduct EDA to understand data distribution, correlations, and key insights.\n",
    "   - Visualize important patterns and relationships in the data.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Develop and select features that significantly impact credit risk prediction.\n",
    "   - Implement techniques such as one-hot encoding, scaling, and normalization.\n",
    "\n",
    "4. **Model Development**:\n",
    "   - Train and compare multiple machine learning models, including Logistic Regression, Decision Trees, Random Forests, Gradient Boosting Machines, XGBoost, and Neural Networks.\n",
    "   - Perform hyperparameter tuning to optimize model performance.\n",
    "\n",
    "5. **Model Evaluation**:\n",
    "   - Evaluate models using metrics such as AUC-ROC, Precision-Recall, F1 Score, and Confusion Matrix.\n",
    "   - Ensure model interpretability using SHAP values or LIME.\n",
    "\n",
    "6. **Deployment**:\n",
    "   - Develop an API for the model using Flask or FastAPI.\n",
    "   - Deploy the model on a cloud platform (e.g., AWS, GCP, Azure) and integrate with a front-end application.\n",
    "\n",
    "7. **Monitoring and Maintenance**:\n",
    "   - Implement monitoring tools to track model performance over time.\n",
    "   - Set up a pipeline for continuous learning and model updates based on new data.\n",
    "\n",
    "#### Expected Impact\n",
    "\n",
    "By developing a sophisticated credit risk scoring system, this project aims to:\n",
    "- Enhance the accuracy and reliability of credit risk assessments.\n",
    "- Enable financial institutions to make more informed lending decisions.\n",
    "- Reduce the risk of defaults and financial losses.\n",
    "- Promote fair and unbiased credit evaluation processes.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "This project will push the limits of data science in the fintech space by leveraging advanced machine learning techniques to address a critical challenge in credit risk assessment. The outcomes will not only benefit financial institutions but also contribute to the broader goal of financial inclusion and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT NECESSARY MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing libraries for data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Importing libraries for machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Importing libraries for model evaluation\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "# Importing libraries for model interpretability\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Importing libraries for API development and deployment\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "\n",
    "# Miscellaneous libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT THE TRAIN AND TEST CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training set\n",
    "train_df = pd.read_csv(r\"C:\\Users\\Black Concept\\WorkSpace\\ALTSCHOOL\\Datasets\\application_train.csv\")\n",
    "\n",
    "# Import test set\n",
    "test_df = pd.read_csv(r\"C:\\Users\\Black Concept\\WorkSpace\\ALTSCHOOL\\Datasets\\application_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Cleaning: Handle missing values, outliers, and data inconsistencies.\n",
    "\n",
    "  Normalization/Standardization: Scale numerical features.\n",
    "\n",
    "  Encoding: Convert categorical variables into numerical values using techniques like one-hot encoding or label encoding.\n",
    "\n",
    "  Feature Engineering: Create new features based on domain knowledge (e.g., debt-to-income ratio, credit utilization rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project already broke down the dataset we are using into train and test set already which is a good approach so we can take a look at the two datasets separately during exploratory data analysis where we take a look at the data- the various connections between features and distribution of features as well.\n",
    "\n",
    "Taking a look at the training and test dataset is very import to avoid bias which could lead to overfitting and again, its important to see that our features are well represented and distributed evenly across the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How large is the dataset that we are working with and what are the different features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 307511 rows (observations) and 122 columns (features)\n",
      "The test set has 48744 rows (observations) and 121 columns (features)\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the datasets\n",
    "train_size = train_df.shape # Training size\n",
    "test_size = test_df.shape # Test size\n",
    "\n",
    "print(f'The training set has {train_size[0]} rows (observations) and {train_size[-1]} columns (features)')\n",
    "\n",
    "print(f'The test set has {test_size[0]} rows (observations) and {test_size[-1]} columns (features)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER',\n",
       "       'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL',\n",
       "       'AMT_CREDIT', 'AMT_ANNUITY',\n",
       "       ...\n",
       "       'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n",
       "       'FLAG_DOCUMENT_21', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
       "       'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
       "       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
       "       'AMT_REQ_CREDIT_BUREAU_YEAR'],\n",
       "      dtype='object', length=122)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the different columns and their respective descriptions\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
